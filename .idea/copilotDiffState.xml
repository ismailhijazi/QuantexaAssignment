<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/ARCHITECTURE.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/ARCHITECTURE.md" />
              <option name="originalContent" value="# QuantexaAssignment Project Architecture&#10;&#10;## 1. Introduction &amp; Overview&#10;QuantexaAssignment is a Spark/Scala project for flight and passenger analytics, written in a functional style and following best practices for modularity, clarity, and maintainability. The solution leverages Spark for distributed, scalable analytics and uses the Scala standard library where appropriate. All analytics modules are implemented in Scala, with careful consideration for scalability and performance, making the solution suitable for both small and very large datasets. Robust configuration is provided via Typesafe Config, ensuring flexibility and ease of extension.&#10;&#10;## 2. Technologies &amp; Dependencies&#10;- **Language:** Scala 2.12.10&#10;- **Framework:** Apache Spark 2.4.8&#10;- **Build Tool:** SBT&#10;- **Config:** Typesafe Config&#10;- **Testing:** ScalaTest&#10;- **Logging:** Log4j&#10;- **Data Format:** CSV, Parquet (added for performance)&#10;&#10;## 3. Directory Structure&#10;```&#10;SbtExampleProject/&#10;├── build.sbt&#10;├── README.md&#10;├── data/&#10;│   ├── flightData.csv&#10;│   └── passengers.csv&#10;├── results/&#10;│   └── [output CSVs]&#10;├── src/&#10;│   ├── main/&#10;│   │   ├── resources/&#10;│   │   │   └── application.conf&#10;│   │   └── scala/&#10;│   │       ├── Main.scala&#10;│   │       ├── analytics/&#10;│   │       ├── model/&#10;│   │       └── util/&#10;│   └── test/&#10;│       └── scala/&#10;│           ├── analytics/&#10;│           ├── util/&#10;└── target/&#10;```&#10;- **main/scala:** Source code (analytics, models, utilities)&#10;- **test/scala:** Unit tests&#10;- **resources:** Configuration files&#10;- **data:** Input CSVs&#10;- **results:** Output CSVs&#10;&#10;## 4. Configuration System&#10;- Managed via `application.conf` (Typesafe Config)&#10;- All config keys documented in README.md&#10;- Example:&#10;```hocon&#10;quantexa {&#10;  input {&#10;    flights = &quot;data/flightData.csv&quot;&#10;    passengers = &quot;data/passengers.csv&quot;&#10;  }&#10;  output {&#10;    mostFrequentFlyers = &quot;results/MostFrequentFlyers.csv&quot;&#10;    singleCsv = false&#10;  }&#10;  spark {&#10;    master = &quot;local[*]&quot;&#10;    appName = &quot;Quantexa Flight Stats Functional&quot;&#10;    spark.executor.memory = &quot;16g&quot;  # Increased for big data&#10;    spark.executor.cores = 3&#10;    spark.default.parallelism = 32  # Increased for better parallelism&#10;    spark.sql.shuffle.partitions = 200  # Increased for better shuffle performance&#10;    spark.serializer = &quot;org.apache.spark.serializer.KryoSerializer&quot;  # Kryo for faster serialization&#10;    spark.sql.autoBroadcastJoinThreshold = &quot;200MB&quot;  # Enable broadcast joins for small tables&#10;  }&#10;}&#10;```&#10;&#10;## 5. Main Components&#10;### Data Models&#10;- **Flight:** passengerId, flightId, from, to, date&#10;- **Passenger:** passengerId, firstName, lastName&#10;- Other analytics-specific models (e.g., FrequentFlyer, LongestRun)&#10;&#10;### Analytics Modules&#10;- **MostFrequentFlyers:** Top N frequent flyers&#10;- **LongestRunWithoutUK:** Longest sequence of flights not visiting UK&#10;- **PassengersFlownTogether:** Pairs flown together N times (now uses optimized DataFrame API for performance; legacy RDD method enhanced for efficiency)&#10;- **FlownTogetherInDateRange:** Pairs flown together N times in date range&#10;- **TotalFlightsPerMonth:** Monthly flight counts&#10;&#10;### Utilities&#10;- **CsvUtils:** Read/write CSVs, single-file or directory output. Now avoids coalesce(1) bottleneck for better performance.&#10;- **ParquetUtils:** Read/write Parquet files for high-performance analytics.&#10;&#10;## 6. Data Flow &amp; Processing&#10;```mermaid&#10;flowchart TD&#10;    A[CSV Input Files] --&gt; B[CsvUtils.safeReadCsv]&#10;    B --&gt; C[Analytics Module]&#10;    C --&gt; D[CsvUtils.saveAsCsv or ParquetUtils.saveAsParquet]&#10;    D --&gt; E[Output CSV (single file or directory)]&#10;```&#10;- Data is read from CSVs, processed by analytics modules, and written to output as per config.&#10;&#10;## 7. Error Handling &amp; Logging&#10;- Uses Try for error handling in data loading and analytics&#10;- Logs errors and info via Log4j&#10;- Handles missing files, schema mismatches, empty datasets&#10;&#10;## 8. Testing Strategy&#10;- Unit tests for all analytics and utilities (ScalaTest)&#10;- Data validation tests: schema checks, missing columns, nulls, invalid data&#10;- Edge cases: empty files, missing files, schema errors&#10;- Test reports in `target/test-reports/`&#10;&#10;## 9. Extensibility &amp; Maintenance&#10;- Add new analytics by creating new objects in `analytics/`&#10;- Add new config keys in `application.conf` and document in README.md&#10;- Utilities are reusable and modular&#10;&#10;## 10. Example Workflow&#10;1. Place input CSVs in `data/`&#10;2. (Optional) Convert CSVs to Parquet for best performance using ParquetUtils&#10;3. Configure `application.conf` (see below for new keys)&#10;4. Run analytics via SBT:&#10;   ```sh&#10;   sbt &quot;run MostFrequentFlyers 100 --show&quot;&#10;   ```&#10;5. Check results in `results/`&#10;" />
              <option name="updatedContent" value="# QuantexaAssignment Project Architecture&#13;&#10;&#13;&#10;## 1. Introduction &amp; Overview&#13;&#10;QuantexaAssignment is a Spark/Scala project for flight and passenger analytics, written in a functional style and following best practices for modularity, clarity, and maintainability. The solution leverages Spark for distributed, scalable analytics and uses the Scala standard library where appropriate. All analytics modules are implemented in Scala, with careful consideration for scalability and performance, making the solution suitable for both small and very large datasets. Robust configuration is provided via Typesafe Config, ensuring flexibility and ease of extension.&#13;&#10;&#13;&#10;## 2. Technologies &amp; Dependencies&#13;&#10;- **Language:** Scala 2.12.10&#13;&#10;- **Framework:** Apache Spark 2.4.8&#13;&#10;- **Build Tool:** SBT&#13;&#10;- **Config:** Typesafe Config&#13;&#10;- **Testing:** ScalaTest&#13;&#10;- **Logging:** Log4j&#13;&#10;- **Data Format:** CSV, Parquet (added for performance)&#13;&#10;&#13;&#10;## 3. Directory Structure&#13;&#10;```&#13;&#10;SbtExampleProject/&#13;&#10;├── build.sbt&#13;&#10;├── README.md&#13;&#10;├── data/&#13;&#10;│   ├── flightData.csv&#13;&#10;│   └── passengers.csv&#13;&#10;├── results/&#13;&#10;│   └── [output CSVs]&#13;&#10;├── src/&#13;&#10;│   ├── main/&#13;&#10;│   │   ├── resources/&#13;&#10;│   │   │   └── application.conf&#13;&#10;│   │   └── scala/&#13;&#10;│   │       ├── Main.scala&#13;&#10;│   │       ├── analytics/&#13;&#10;│   │       ├── model/&#13;&#10;│   │       └── util/&#13;&#10;│   └── test/&#13;&#10;│       └── scala/&#13;&#10;│           ├── analytics/&#13;&#10;│           ├── util/&#13;&#10;└── target/&#13;&#10;```&#13;&#10;- **main/scala:** Source code (analytics, models, utilities)&#13;&#10;- **test/scala:** Unit tests&#13;&#10;- **resources:** Configuration files&#13;&#10;- **data:** Input CSVs&#13;&#10;- **results:** Output CSVs&#13;&#10;&#13;&#10;## 4. Configuration System&#13;&#10;- Managed via `application.conf` (Typesafe Config)&#13;&#10;- All config keys documented in README.md&#13;&#10;- Example:&#13;&#10;```hocon&#13;&#10;quantexa.input.flights = &quot;data/flightData.csv&quot;&#13;&#10;quantexa.input.passengers = &quot;data/passengers.csv&quot;&#13;&#10;quantexa.output.mostFrequentFlyers = &quot;results/MostFrequentFlyers.csv&quot;&#13;&#10;quantexa.output.singleCsv = true&#13;&#10;quantexa.spark.master = &quot;local[*]&quot;&#13;&#10;quantexa.spark.appName = &quot;QuantexaAssignment&quot;&#13;&#10;quantexa.spark.executor.memory = &quot;16g&quot;&#13;&#10;quantexa.spark.executor.cores = 3&#13;&#10;quantexa.spark.default.parallelism = 32&#13;&#10;quantexa.spark.sql.shuffle.partitions = 200&#13;&#10;quantexa.spark.serializer = &quot;org.apache.spark.serializer.KryoSerializer&quot;&#13;&#10;quantexa.spark.sql.autoBroadcastJoinThreshold = &quot;200MB&quot;&#13;&#10;```&#13;&#10;&#13;&#10;## 5. Analytics Orchestration&#13;&#10;- The main workflow is managed by `Main.scala`, which loads configuration, validates input data, and runs the selected analytics module.&#13;&#10;- Data validation is performed before analytics to ensure schema and non-empty datasets.&#13;&#10;- Results are written to CSV (or Parquet for large data) as configured.&#13;&#10;&#13;&#10;## 6. Testing&#13;&#10;- Unit tests are located in `src/test/scala/analytics/` and `src/test/scala/util/`.&#13;&#10;- Tests cover utility functions and analytics modules for correctness and robustness.&#13;&#10;&#13;&#10;## 7. Error Handling &amp; Logging&#13;&#10;- Uses Try for error handling in data loading and analytics&#13;&#10;- Logs errors and info via Log4j&#13;&#10;- Handles missing files, schema mismatches, empty datasets&#13;&#10;&#13;&#10;## 8. Extensibility &amp; Maintenance&#13;&#10;- Add new analytics by creating new objects in `analytics/`&#13;&#10;- Add new config keys in `application.conf` and document in README.md&#13;&#10;- Utilities are reusable and modular&#13;&#10;&#13;&#10;## 9. Example Workflow&#13;&#10;1. Place input CSVs in `data/`&#13;&#10;2. (Optional) Convert CSVs to Parquet for best performance using ParquetUtils&#13;&#10;3. Configure `application.conf` (see below for new keys)&#13;&#10;4. Run analytics via SBT:&#13;&#10;   ```sh&#13;&#10;   sbt &quot;run MostFrequentFlyers 100 --show&quot;&#13;&#10;   ```&#13;&#10;5. Check results in `results/`" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/BIG_DATA_ENHANCEMENTS.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/BIG_DATA_ENHANCEMENTS.md" />
              <option name="originalContent" value="# Big Data Enhancements Summary&#10;&#10;This document outlines the key enhancements made to optimize the QuantexaAssignment project for big data scenarios using Spark/Scala.&#10;&#10;## 1. Configuration Improvements&#10;- **Increased Spark Executor Memory:**&#10;  - `spark.executor.memory = &quot;16g&quot;` &#10;- **Higher Parallelism:**&#10;  - `spark.default.parallelism = 32` &#10;  - `spark.sql.shuffle.partitions = 200` &#10;- **Kryo Serialization:**&#10;  - `spark.serializer = &quot;org.apache.spark.serializer.KryoSerializer&quot;`&#10;- **Broadcast Join Optimization:**&#10;  - `spark.sql.autoBroadcastJoinThreshold = &quot;200MB&quot;`&#10;- **Output Handling:**&#10;  - `singleCsv = false` to avoid bottlenecks and enable parallel writes&#10;&#10;## 2. Code-Level Enhancements&#10;- **CsvUtils:**&#10;  - Avoids `coalesce(1)` for large outputs, enabling parallel file writes.&#10;  - Warns if single file output is requested for big data.&#10;- **ParquetUtils:**&#10;  - Added for efficient columnar storage and fast reads/writes.&#10;  - Supports conversion from CSV to Parquet and compressed output.&#10;- **PassengersFlownTogether:**&#10;  - Refactored to use DataFrame API for much faster performance and scalability.&#10;  - Legacy RDD method enhanced for efficiency if needed.&#10;- **DataValidationUtils:**&#10;  - Validates input data for schema correctness and non-empty datasets before analytics are run.&#10;  - Provides extensible data quality rules (e.g., duplicate detection).&#10;  - Integrated in the main workflow to ensure robust data quality and prevent downstream errors.&#10;  - Located at `src/main/scala/util/DataValidationUtils.scala`.&#10;&#10;## 3. Architectural &amp; Workflow Enhancements&#10;- **Functional Programming Best Practices:**&#10;  - Immutable data structures, pure functions, and modular analytics objects.&#10;- **Modular Utilities:**&#10;  - Clear separation of concerns: CsvUtils for CSV, ParquetUtils for Parquet.&#10;- **Flexible Configuration:**&#10;  - Uses Typesafe Config for easy tuning and environment adaptation.&#10;- **Documentation:**&#10;  - README.md and ARCHITECTURE.md updated to reflect all enhancements and usage patterns.&#10;&#10;## 4. Best Practices &amp; Recommendations&#10;- **Use Parquet for Large Datasets:**&#10;  - Parquet files are faster to read (5-10x) and take up less space (50-80% smaller) compared to CSVs.&#10;- **Avoid Single File Output for Big Data:**&#10;  - Instead of writing everything to one file, let Spark create multiple smaller files. This makes the process faster and more scalable.&#10;- **Leverage Broadcast Joins:**&#10;  - If one side of your join is small (less than 200MB), use broadcast joins to speed things up.&#10;- **Tune Partitioning:**&#10;  - Adjust the number of partitions and parallelism settings to match your cluster size and data volume.&#10;- **Monitor Resource Usage:**&#10;  - Keep an eye on Spark's UI to spot bottlenecks. Increase executor memory if needed.&#10;&#10;## 5. Future Enhancements&#10;- **Add Real-Time Analytics:**&#10;  - Use Spark Streaming to handle data as it comes in, instead of processing it in batches.&#10;- **Set Up Automated Testing and Deployment:**&#10;  - Add CI/CD pipelines to make testing and deployment smoother and faster.&#10;- **Compress Output Files:**&#10;  - Save storage space and speed up file operations by compressing output files with formats like Gzip or Snappy.&#10;" />
              <option name="updatedContent" value="# Big Data Enhancements Summary&#13;&#10;&#13;&#10;This document outlines the key enhancements made to optimize the QuantexaAssignment project for big data scenarios using Spark/Scala.&#13;&#10;&#13;&#10;## 1. Configuration Improvements&#13;&#10;- **Increased Spark Executor Memory:**&#13;&#10;  - `spark.executor.memory = &quot;16g&quot;` &#13;&#10;- **Higher Parallelism:**&#13;&#10;  - `spark.default.parallelism = 32` &#13;&#10;  - `spark.sql.shuffle.partitions = 200` &#13;&#10;- **Kryo Serialization:**&#13;&#10;  - `spark.serializer = &quot;org.apache.spark.serializer.KryoSerializer&quot;`&#13;&#10;- **Broadcast Join Optimization:**&#13;&#10;  - `spark.sql.autoBroadcastJoinThreshold = &quot;200MB&quot;`&#13;&#10;- **Output Handling:**&#13;&#10;  - `singleCsv = false` to avoid bottlenecks and enable parallel writes&#13;&#10;&#13;&#10;## 2. Code-Level Enhancements&#13;&#10;- **CsvUtils:**&#13;&#10;  - Avoids `coalesce(1)` for large outputs, enabling parallel file writes.&#13;&#10;  - Warns if single file output is requested for big data.&#13;&#10;- **ParquetUtils:**&#13;&#10;  - Added for efficient columnar storage and fast reads/writes.&#13;&#10;  - Supports conversion from CSV to Parquet and compressed output.&#13;&#10;  - **Usage Example:**&#13;&#10;    ```scala&#13;&#10;    import QuantexaAssignment.util.ParquetUtils&#13;&#10;    ParquetUtils.convertCsvToParquet(spark, &quot;data/flightData.csv&quot;, &quot;data/flightData.parquet&quot;, schema)&#13;&#10;    ```&#13;&#10;- **PassengersFlownTogether:**&#13;&#10;  - Refactored to use DataFrame API for much faster performance and scalability.&#13;&#10;  - Legacy RDD method enhanced for efficiency if needed.&#13;&#10;- **DataValidationUtils:**&#13;&#10;  - Validates input data for schema correctness and non-empty datasets before analytics are run.&#13;&#10;  - Provides extensible data quality rules (e.g., duplicate detection).&#13;&#10;  - Integrated in the main workflow to ensure robust data quality and prevent downstream errors.&#13;&#10;  - Located at `src/main/scala/util/DataValidationUtils.scala`.&#13;&#10;&#13;&#10;## 3. Architectural &amp; Workflow Enhancements&#13;&#10;- **Functional Programming Best Practices:**&#13;&#10;  - Immutable data structures, pure functions, and modular analytics objects.&#13;&#10;- **Modular Utilities:**&#13;&#10;  - Clear separation of concerns: CsvUtils for CSV, ParquetUtils for Parquet.&#13;&#10;- **Flexible Configuration:**&#13;&#10;  - Uses Typesafe Config for easy tuning and environment adaptation.&#13;&#10;  - **Switching Output Mode:** Set `quantexa.output.singleCsv = false` in `application.conf` to enable parallel output files for big data.&#13;&#10;- **Documentation:**&#13;&#10;  - README.md and ARCHITECTURE.md updated to reflect all enhancements and usage patterns.&#13;&#10;  - For more details, see the referenced documentation files.&#13;&#10;&#13;&#10;## 4. Best Practices &amp; Recommendations&#13;&#10;- **Use Parquet for Large Datasets:**&#13;&#10;  - Parquet files are faster to read (5-10x) and take up less space (50-80% smaller) compared to CSVs.&#13;&#10;- **Avoid Single File Output for Big Data:**&#13;&#10;  - Instead of writing everything to one file, let Spark create multiple smaller files. This makes the process faster and more scalable.&#13;&#10;- **Leverage Broadcast Joins:**&#13;&#10;  - If one side of your join is small (less than 200MB), use broadcast joins to speed things up.&#13;&#10;- **Tune Partitioning:**&#13;&#10;  - Adjust the number of partitions and parallelism settings to match your cluster size and data volume.&#13;&#10;- **Monitor Resource Usage:**&#13;&#10;  - Keep an eye on Spark's UI to spot bottlenecks. Increase executor memory if needed.&#13;&#10;&#13;&#10;## 5. Future Enhancements&#13;&#10;- **Add Real-Time Analytics:**&#13;&#10;  - Use Spark Streaming to handle data as it comes in, instead of processing it in batches.&#13;&#10;- **Set Up Automated Testing and Deployment:**&#13;&#10;  - Add CI/CD pipelines to make testing and deployment smoother and faster.&#13;&#10;- **Compress Output Files:**&#13;&#10;  - Save storage space and speed up file operations by compressing output files with formats like Gzip or Snappy." />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/README.md">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/README.md" />
              <option name="originalContent" value="# QuantexaAssignment&#10;&#10;## Overview&#10;This project solves the Quantexa coding assignment using Spark and Scala in a functional programming style. It processes flight and passenger data to answer four analytics questions and provides results as CSV files.&#10;&#10;## Prerequisites&#10;- Java 8+&#10;- Scala 2.12+&#10;- SBT (Scala Build Tool)&#10;- Spark (included as dependency)&#10;&#10;## Data Files&#10;Place the following CSV files in the `data/` directory:&#10;- `flightData.csv`&#10;- `passengers.csv`&#10;&#10;## How to Run&#10;1. Open a terminal in the project root directory.&#10;2. To run analytics, use:&#10;   ```&#10;   sbt &quot;run &lt;FunctionName&gt; [params] [--show]&quot;&#10;   ```&#10;   - `&lt;FunctionName&gt;`: One of the following:&#10;     - `TotalFlightsPerMonth` (Q1)&#10;     - `MostFrequentFlyers` (Q2)&#10;     - `LongestRunWithoutUK` (Q3)&#10;     - `PassengersFlownTogether` (Q4)&#10;     - `FlownTogetherInDateRange` (Bonus)&#10;   - `[params]`: Optional parameters for some functions (see below).&#10;   - `--show`: Display results in the console.&#10;&#10;### Example Commands&#10;- Q1: Total flights per month&#10;  ```&#10;  sbt &quot;run TotalFlightsPerMonth --show&quot;&#10;  ```&#10;- Q2: 100 most frequent flyers&#10;  ```&#10;  sbt &quot;run MostFrequentFlyers --show&quot;&#10;  ```&#10;- Q3: Longest run without UK (ordered by longest run descending)&#10;  ```&#10;  sbt &quot;run LongestRunWithoutUK --show&quot;&#10;  ```&#10;- Q4: Passengers with &gt;3 flights together (ordered by flights together descending)&#10;  ```&#10;  sbt &quot;run PassengersFlownTogether --show&quot;&#10;  ```&#10;- Bonus: Passengers with &gt;N flights together in date range&#10;  ```&#10;  sbt &quot;run FlownTogetherInDateRange &lt;atLeastNTimes&gt; &lt;fromDate&gt; &lt;toDate&gt; --show&quot;&#10;  # Example:&#10;  sbt &quot;run FlownTogetherInDateRange 5 2017-01-01 2017-03-01 --show&quot;&#10;  ```&#10;&#10;## Output&#10;Results are saved as CSV files in the `results/` directory:&#10;- `TotalFlightsPerMonth.csv`&#10;- `MostFrequentFlyers.csv`&#10;- `LongestRunWithoutUK.csv`&#10;- `PassengersFlownTogether.csv`&#10;- `FlownTogetherInDateRange.csv` (bonus)&#10;&#10;## How to Run Tests&#10;To run unit tests:&#10;```&#10;sbt test&#10;```&#10;Test reports are generated in `target/test-reports/`.&#10;&#10;## Packaging for Submission&#10;Zip the entire project directory, including:&#10;- Source code&#10;- Output CSV files&#10;- README.md&#10;- .git directory (if using Git)&#10;- Test reports&#10;&#10;## Functional Programming Features&#10;- Modularized code using functions and objects&#10;- Typed data using case classes and Datasets&#10;- Immutable values&#10;- Clean, readable code with camelCase naming&#10;- ScalaDocs on public functions and classes&#10;&#10;## Configuration (`application.conf`)&#10;The project uses a configuration file at `src/main/resources/application.conf` to manage input/output paths and Spark settings. You can adjust these parameters to fit your environment or requirements.&#10;&#10;### Parameters&#10;- **quantexa.input.flights**: Path to the flight data CSV file (default: `data/flightData.csv`).&#10;- **quantexa.input.passengers**: Path to the passenger data CSV file (default: `data/passengers.csv`).&#10;- **quantexa.output.mostFrequentFlyers**: Path for the output CSV file for most frequent flyers (default: `results/MostFrequentFlyers.csv`).&#10;- **quantexa.output.singleCsv**: If `true`, outputs all results to a single CSV file (default: `true`).&#10;- **quantexa.spark.master**: Spark master URL (default: `local[*]` for local execution).&#10;- **quantexa.spark.appName**: Spark application name.&#10;- **quantexa.spark.executor.memory**: Memory allocated per Spark executor (default: `16g`).&#10;- **quantexa.spark.executor.cores**: Number of cores per Spark executor (default: `3`).&#10;- **quantexa.spark.default.parallelism**: Default parallelism for Spark jobs (default: `32`).&#10;- **quantexa.spark.sql.shuffle.partitions**: Number of partitions for shuffle operations (default: `200`).&#10;- **quantexa.spark.serializer**: Serializer used by Spark (default: `KryoSerializer` for performance).&#10;- **quantexa.spark.sql.autoBroadcastJoinThreshold**: Threshold for broadcast joins (default: `200MB`).&#10;&#10;### How to Change Configuration&#10;Edit `src/main/resources/application.conf` to change file paths or Spark settings. For example, to use a different input file, update:&#10;```&#10;quantexa.input.flights = &quot;data/myFlights.csv&quot;&#10;```&#10;&#10;### Notes&#10;- For large datasets, you may want to increase memory, cores, or parallelism.&#10;- The configuration is loaded automatically when you run the project.&#10;- All output files are saved to the paths specified in this file.&#10;&#10;## Contact&#10;For any questions, please contact the author or refer to the assignment instructions.&#10;" />
              <option name="updatedContent" value="# QuantexaAssignment&#10;&#10;## Overview&#10;This project solves the Quantexa coding assignment using Spark and Scala in a functional programming style. It processes flight and passenger data to answer four analytics questions and provides results as CSV files.&#10;&#10;## Prerequisites&#10;- Java 8+&#10;- Scala 2.12+&#10;- SBT (Scala Build Tool)&#10;- Spark (included as dependency)&#10;&#10;## Data Files&#10;Place the following CSV files in the `data/` directory:&#10;- `flightData.csv`&#10;- `passengers.csv`&#10;&#10;## How to Run&#10;1. Open a terminal in the project root directory.&#10;2. To run analytics, use:&#10;   ```&#10;   sbt &quot;run &lt;FunctionName&gt; [params] [--show]&quot;&#10;   ```&#10;   - `&lt;FunctionName&gt;`: One of the following:&#10;     - `TotalFlightsPerMonth` (Q1)&#10;     - `MostFrequentFlyers` (Q2)&#10;     - `LongestRunWithoutUK` (Q3)&#10;     - `PassengersFlownTogether` (Q4)&#10;     - `FlownTogetherInDateRange` (Bonus)&#10;   - `[params]`: Optional parameters for some functions (see below).&#10;   - `--show`: Display results in the console.&#10;&#10;### Example Commands&#10;- Q1: Total flights per month&#10;  ```&#10;  sbt &quot;run TotalFlightsPerMonth --show&quot;&#10;  ```&#10;- Q2: 100 most frequent flyers&#10;  ```&#10;  sbt &quot;run MostFrequentFlyers --show&quot;&#10;  ```&#10;- Q3: Longest run without UK (ordered by longest run descending)&#10;  ```&#10;  sbt &quot;run LongestRunWithoutUK --show&quot;&#10;  ```&#10;- Q4: Passengers with &gt;3 flights together (ordered by flights together descending)&#10;  ```&#10;  sbt &quot;run PassengersFlownTogether --show&quot;&#10;  ```&#10;- Bonus: Passengers with &gt;N flights together in date range&#10;  ```&#10;  sbt &quot;run FlownTogetherInDateRange &lt;atLeastNTimes&gt; &lt;fromDate&gt; &lt;toDate&gt; --show&quot;&#10;  # Example:&#10;  sbt &quot;run FlownTogetherInDateRange 5 2017-01-01 2017-03-01 --show&quot;&#10;  ```&#10;&#10;## Output&#10;Results are saved as CSV files in the `results/` directory:&#10;- `TotalFlightsPerMonth.csv`&#10;- `MostFrequentFlyers.csv`&#10;- `LongestRunWithoutUK.csv`&#10;- `PassengersFlownTogether.csv`&#10;- `FlownTogetherInDateRange.csv` (bonus)&#10;&#10;## How to Run Tests&#10;To run unit tests:&#10;```&#10;sbt test&#10;```&#10;Test reports are generated in `target/test-reports/`.&#10;&#10;## Testing Strategy&#10;Unit tests are provided for both utility and analytics modules. The tests cover:&#10;- Data validation (schema, non-empty datasets)&#10;- CSV and Parquet reading/writing&#10;- Analytics logic for all assignment questions&#10;&#10;Tests are written using ScalaTest and can be run with `sbt test`. Test reports are generated in `target/test-reports/` for review. This ensures correctness, robustness, and maintainability of the codebase.&#10;&#10;In addition to automated unit tests, manual testing was performed using a small sample of flight and passenger data. This helped verify the correctness of analytics outputs and ensured the application works as expected with real input files and edge cases.&#10;&#10;## Packaging for Submission&#10;Zip the entire project directory, including:&#10;- Source code&#10;- Output CSV files&#10;- README.md&#10;- Test reports&#10;- ARCHITECTURE.md&#10;- BIG_DATA_ENHANCEMENTS.md&#10;&#10;## Functional Programming Features&#10;- Modularized code using functions and objects&#10;- Typed data using case classes and Datasets&#10;- Immutable values&#10;- Clean, readable code with camelCase naming&#10;- ScalaDocs on public functions and classes&#10;&#10;## Configuration (`application.conf`)&#10;The project uses a configuration file at `src/main/resources/application.conf` to manage input/output paths and Spark settings. You can adjust these parameters to fit your environment or requirements.&#10;&#10;### Parameters&#10;- **quantexa.input.flights**: Path to the flight data CSV file (default: `data/flightData.csv`).&#10;- **quantexa.input.passengers**: Path to the passenger data CSV file (default: `data/passengers.csv`).&#10;- **quantexa.output.mostFrequentFlyers**: Path for the output CSV file for most frequent flyers (default: `results/MostFrequentFlyers.csv`).&#10;- **quantexa.output.singleCsv**: If `true`, outputs all results to a single CSV file (default: `true`).&#10;- **quantexa.spark.master**: Spark master URL (default: `local[*]` for local execution).&#10;- **quantexa.spark.appName**: Spark application name.&#10;- **quantexa.spark.executor.memory**: Memory allocated per Spark executor (default: `16g`).&#10;- **quantexa.spark.executor.cores**: Number of cores per Spark executor (default: `3`).&#10;- **quantexa.spark.default.parallelism**: Default parallelism for Spark jobs (default: `32`).&#10;- **quantexa.spark.sql.shuffle.partitions**: Number of partitions for shuffle operations (default: `200`).&#10;- **quantexa.spark.serializer**: Serializer used by Spark (default: `KryoSerializer` for performance).&#10;- **quantexa.spark.sql.autoBroadcastJoinThreshold**: Threshold for broadcast joins (default: `200MB`).&#10;&#10;### How to Change Configuration&#10;Edit `src/main/resources/application.conf` to change file paths or Spark settings. For example, to use a different input file, update:&#10;```&#10;quantexa.input.flights = &quot;data/myFlights.csv&quot;&#10;```&#10;&#10;### Notes&#10;- For large datasets, you may want to increase memory, cores, or parallelism.&#10;- The configuration is loaded automatically when you run the project.&#10;- All output files are saved to the paths specified in this file.&#10;&#10;## Further Documentation&#10;For more details about the project structure and scalability enhancements, refer to:&#10;- **ARCHITECTURE.md**: Overview of the system architecture, design decisions, and module interactions.&#10;- **BIG_DATA_ENHANCEMENTS.md**: Description of optimizations and enhancements for handling large datasets and improving Spark performance.&#10;&#10;## Contact&#10;For any questions, please contact the author or refer to the assignment instructions." />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>